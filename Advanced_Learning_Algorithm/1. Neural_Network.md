# Neural Networks Intuition

## 1. Neurons and the brain

 ### 1.1 Neural networks

origins: Algorithms that try to mimic the brain.



used in the 1980's and early 1990's

Fell out of favor in the late 1990's



Resurgence from around 2005



speech $\rightarrow$ images $\rightarrow$ text (NLP)



### 1.2 Neurons in the brain

 Simplified mathematical model of a neuron

Artificial neural networks do not actually mimic the complexity of real biological neurons



### 1.3 Why Now?

![](/Users/linyin/study/machine_learning/img/Neurons_and_the_brain.png)

## 2. Demand Prediction

### 2.1 Demand predictions

![](/Users/linyin/study/machine_learning/img/deamnd_prediction_2.png)

when you train it from the data,

 the neural network should compute the features it wants to use in this hiddle layer.   

### 2.2 Multiple hidden layers

![](/Users/linyin/study/machine_learning/img/multiple_hidden_layers.png)



Neural Network Architecture:

decision you need to make,

How many hidden layers do you want 

and how many neurons do you want each hidden layer to have

## 3. Example: Recognizing Images

### 3.1 Face recognition

![](/Users/linyin/study/machine_learning/img/face_recognition.png)

first hidden layer: look for a small window

second hidden layer: look for a big window

third hidden layer: look for a even big window



so the activations are higher level features!!!



# Neural network model

## 1. Neural network layer

the firt hiddle layer:

​		$a_1^{[1]} = g(\vec{w}^{[1]}_1 \cdot \vec{x} + b^{[1]}_1)$

the second hiddle layer:

​		$a_2^{[1]} = g(\vec{w}^{[2]}_1 \cdot \vec{a}^{[1]} + b^{[2]}_1)$

predict category 1 or 0

​		is $a^{[2]} \geq 0.5 $ ? Yes: $\hat{y} = 1$ : No: $\hat{y} = 0$ 

## 2. More complex neural networks

If we say 4 layers, it contians hiddle layers and output layer.

 But we don;t count the input layer

![](/Users/linyin/study/machine_learning/img/more_complex_neural_network.png)

an Layer $l$ and for an arbitrary unit $j$,
$$
a_j^{[l]} = g(\vec{w}^{[l]}_j \cdot \vec{a}^{[l - 1]} + b^{[l]}_j)
$$
 $g$ is called activation function

note: we call $\vec{x} = \vec{a}^[0]$, so this also works for the first layer



## 3. Inference: making predictions(forward propagation)

we use $f(x)$ to denote the output of linear regression of logistic regression

![](/Users/linyin/study/machine_learning/img/handwritten_digit_recognition.png)

