# Classfication



## 1. Motivation

$y$ can only be one of two values: "binary classfication"



false , 0 = "negative class" $\not=$ "bad"



if $f_{w,b}(x) < 0.5 \rightarrow \hat{y} = 0$

if $f_{w,b}(x) > 0.5 \rightarrow \hat{y} = 1$



## 2. Logistic Regression

### 2.1 sigmoid function OR logistic funciton

![](/Users/linyin/study/machine_learning/img/sigmoid_function.png)

Outputs between 0 and 1


$$
g(z) = \frac{1}{1 + e^{-1}} \quad 0 <g(z)<1 
$$
logistic regression
$$
f_{\vec{w},b}(\vec{x}) = g(\vec{w} \cdot \vec{x} + b) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}
$$

### 2.2 Interpretation of logistic regression output

$f_{\vec{w},b}(\vec{x}) = g(\vec{w} \cdot \vec{x} + b) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}$

"probability" that class is 1

$P(y = 0) + P(y = 1) = 1$



Example:

$x$ is "tumor size"

$y$ is 0 (not malignant)

  or 1 (maligant)

$f_{\vec{w},b}(\vec{x}) = 0.7$

means that 70% chance that $y$ is 1

## 3. Decision boundary

When is $f_{\vec{w},b}(\vec{x}) \geq 0.5$ ?

​	$g(z) \geq 0.5$

​	$z \geq 0.5$

​	$\vec{w} \cdot \vec{x} + b \geq 0$

​	$\hat{y} = 1$

 

Decision boundary $z = \vec{w} \cdot \vec{x} + b = 0$

# Cost function

## 1. Cost fucntion for logistic regression

### 1.1 Squared error cost

$$
J(w,b) = \frac{1}{m}\sum_1^m\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2
$$

linear regression

$f_{\vec{w},b}(\vec{x})=\vec{w} \cdot \vec{x} + b$

logistic regression

$f_{\vec{w},b}(\vec{x}) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}$

![](/Users/linyin/study/machine_learning/img/Squard_error_cost.png)

if you try to use gradient descent, there are lots of local minima that you can get sucking

### 1.2 Logistic loss function

$$
L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = \begin{cases}
    {-log(f_{\vec{w},b}(\vec{x}^{(i)}))} & \text{if} \ \ y^(i)=1 \\
    {-log(1-f_{\vec{w},b}(\vec{x}^{(i)}))} & \text{if} \ \ y^(i)=0 \\
\end{cases}
$$

Loss is lowest when $f_{\vec{w},b}(\vec{x})$ predicts close to true label $y^{(i)}$.

## 2. Simplified Cost Function

​	 
$$
L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = -y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1 - y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})
$$
if $y^{(i)} = 1$:

$L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = -1\times log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1 - 1)log(1-f_{\vec{w},b}(\vec{x}^{(i)}) = -1 \times log(f(\vec{x^{(i)}}))$



if $y^{(i) = 0}$:

$L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = -0\times log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1 - 0)\times log(1-f_{\vec{w},b}(\vec{x}^{(i)}) = -log(1-f_{\vec{w},b}(\vec{x}^{(i)})$



now,
$$
J(\vec{w}, b) = \frac{1}{m}\sum_{i = 1}^m[L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i))]\\
 = -\frac{1}{m}\sum_{i = 1}^m[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) + (1 - y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})]
$$

# Gradient descent

## 1. Gradient descent implementation

 ### 1.1 Training logistic regression

find $\vec{w},b$

Given new $\vec{x}$, output $f_{\vec{w},b}(\vec{x}) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}$
$$
P(y = 1|\vec{x};\vec{w},b)
$$



### 1.2 Gradient descent

$J(\vec{w}, b) = -\frac{1}{m}\sum_{i = 1}^m[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) + (1 - y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})]$

repeat{

​	$w_j = w_j - \alpha \frac{\partial}{\partial w_j}J(\vec{w},b)$

​	$b = b - \alpha \frac{\partial}{\partial b}J(\vec{w},b)$

}

 $\frac{\partial}{\partial w_j}J(\vec{w},b) = \frac{1}{m}\sum_1^m(f_{w, b}(\vec{x}^{(i)})-{y}^{(i)})x^{(i)}_j $

 $\frac{\partial}{\partial b}J(\vec{w},b) = \frac{1}{m}\sum_1^m(f_{w, b}(\vec{x}^{(i)})-{y}^{(i)}) $



 ### 1.3 Gradient descent for logistic regression

repeat{

​	$w_j = w_j - \alpha [\frac{1}{m}\sum_1^m(f_{w, b}(\vec{x}^{(i)})-{y}^{(i)})x^{(i)}_j]$

​	$b = b - \alpha [\frac{1}{m}\sum_1^m(f_{w, b}(\vec{x}^{(i)})-{y}^{(i)})]$

}simultaneously updates



linear regression 	$f_{\vec{w},b}(\vec{x})=\vec{w} \cdot \vec{x} + b$

logistic regression	 $f_{\vec{w},b}(\vec{x}) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}$



# Regularization to Reduce Overfitting

## 1. The problem of Overfitting

### 1.1 Regression example

![](/Users/linyin/study/machine_learning/img/regression_example_1.png)

Underfit / high bias:

Does not fit the training set well



![](/Users/linyin/study/machine_learning/img/regression_example_2.png)

Generalization:

Fits training set pretty well

![](/Users/linyin/study/machine_learning/img/regression_example_3.png)

Overfit / high variance

Fits the training set extremely well



### 1.2 Classification

![](/Users/linyin/study/machine_learning/img/classfication.png)

## 2.  Addressing Overfitting

there are three options

### 2.1 Collet more training example

really helpful, but sometimes impossible

 ### 2.2 Select features to include/exclude

1. not use so many of these polynomial features

2. Features selection:
   if you have a lot of features, but don't have enough training date, the learning algorithm will also be overfit.
   So we can select features.But it will throw away some information

### 2.3 Regularization

 Reduce the size of parameters $w_j$

 use the smaller values for $w_j$

## 3. Cost Function with Regularization

### 3.1 Intution

![](/Users/linyin/study/machine_learning/img/intution.png)

make $w_3, w_4$ really small ($\approx 0$)
$$
min_{\vec{w}, b}\sum^{m}_{i = 1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + 1000w_3^2+1000w_4^2
$$

### 3.2 Regularization

small values $w_1, w_2 \dots, w_n, b$

simpler model less likely to overfit



regularization parameters "lambda": $\lambda > 0$
$$
J(w,b) = \frac{1}{m}\sum_1^m\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum^m_{j = 1}w_j^2
$$
 J with $mean \  squared \ error$ plus $relurization \ term$



1. if $\lambda = 0$, it will be overfitting
2. if $\lambda$ is very large, all the $w_j$ will be smll and $f(x) = b$



## 4. Regularized Linear Regression

### 4.1 Regularized Linear Regression

$$
\min_{\vec{w},b}J(w,b) = \min_{\vec{w},b}[\frac{1}{m}\sum_1^m\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum^m_{j = 1}w_j^2]
$$

Gradient descent

repeat{

​	$w_j = w_j - \alpha \frac{\partial}{\partial w_j}J(\vec{w},b)$

​	$b = b - \alpha \frac{\partial}{\partial b}J(\vec{w},b)$

}simultaneously updates

 $\frac{\partial}{\partial w_j}J(\vec{w},b) = \frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-{y}^{(i)})x^{(i)}_j )+ \frac{\lambda}{m}w_j$

 $\frac{\partial}{\partial b}J(\vec{w},b) = \frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-{y}^{(i)}) $

### 4.2 Implementing gradient descent

repeat{

​	$w_j = w_j - \alpha[\frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-{y}^{(i)})x^{(i)}_j )+ \frac{\lambda}{m}w_j]$

​	$b = b - \alpha[\frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-{y}^{(i)})]$

} simultaneously updates



note: there is no $\sum$ befero the $w_j$, cuz the $\frac{\partial}{\partial{w_j}}$

every $\vec{w}$ has $w_j$, but $\sum_{i = 1}^m$ only has one $w_j$

## 5. Regularized Logistic Regression

### 5.1 Regularized Logistic Regression

 Cost function:
$$
J(\vec{w}, b) = -\frac{1}{m}\sum_{i = 1}^m[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) + (1 - y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})] + \frac{\lambda}{2m}\sum^m_{j = 1}w_j^2
$$

repeat{

​	$w_j = w_j - \alpha[\frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-{y}^{(i)})x^{(i)}_j )+ \frac{\lambda}{m}w_j]$

​	$b = b - \alpha[\frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-{y}^{(i)})]$

} simultaneously updates		
