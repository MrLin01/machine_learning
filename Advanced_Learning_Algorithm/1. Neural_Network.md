# Neural Networks Intuition

## 1. Neurons and the brain

 ### 1.1 Neural networks

origins: Algorithms that try to mimic the brain.



used in the 1980's and early 1990's

Fell out of favor in the late 1990's



Resurgence from around 2005



speech $\rightarrow$ images $\rightarrow$ text (NLP)



### 1.2 Neurons in the brain

 Simplified mathematical model of a neuron

Artificial neural networks do not actually mimic the complexity of real biological neurons



### 1.3 Why Now?

![](/Users/linyin/study/machine_learning/img/Neurons_and_the_brain.png)

## 2. Demand Prediction

### 2.1 Demand predictions

![](/Users/linyin/study/machine_learning/img/deamnd_prediction_2.png)

when you train it from the data,

 the neural network should compute the features it wants to use in this hiddle layer.   

### 2.2 Multiple hidden layers

![](/Users/linyin/study/machine_learning/img/multiple_hidden_layers.png)



Neural Network Architecture:

decision you need to make,

How many hidden layers do you want 

and how many neurons do you want each hidden layer to have

## 3. Example: Recognizing Images

### 3.1 Face recognition

![](/Users/linyin/study/machine_learning/img/face_recognition.png)

first hidden layer: look for a small window

second hidden layer: look for a big window

third hidden layer: look for a even big window



so the activations are higher level features!!!



# Neural network model

## 1. Neural network layer

the firt hiddle layer:

​		$a_1^{[1]} = g(\vec{w}^{[1]}_1 \cdot \vec{x} + b^{[1]}_1)$

the second hiddle layer:

​		$a_2^{[1]} = g(\vec{w}^{[2]}_1 \cdot \vec{a}^{[1]} + b^{[2]}_1)$

predict category 1 or 0

​		is $a^{[2]} \geq 0.5 $ ? Yes: $\hat{y} = 1$ : No: $\hat{y} = 0$ 

## 2. More complex neural networks

If we say 4 layers, it contians hiddle layers and output layer.

 But we don;t count the input layer

![](/Users/linyin/study/machine_learning/img/more_complex_neural_network.png)

an Layer $l$ and for an arbitrary unit $j$,
$$
a_j^{[l]} = g(\vec{w}^{[l]}_j \cdot \vec{a}^{[l - 1]} + b^{[l]}_j)
$$
 $g$ is called activation function

note: we call $\vec{x} = \vec{a}^[0]$, so this also works for the first layer



## 3. Inference: making predictions(forward propagation)

we use $f(x)$ to denote the output of linear regression of logistic regression

![](/Users/linyin/study/machine_learning/img/handwritten_digit_recognition.png)

# Tensorflow implementation

## 1.  Inference in Code

````python
x = np.array([[200.0, 17.0]])
layer_1 = Dense(units = 3, activation = 'sigmoid')
a1 = layer_1(x)

layer_2 = Dense(units = 1, activation = 'sigmoid')
a2 = layer_2(a1)

if a2 >= 0.5:
	yhat = 1
else:
  yhat = 0
````



## 2. Data in TensorFlow

### 2.1 Note about numpy arrarys

```python
x = np.array([[200, 17]]) # 1 * 2 【200， 17】
x = np.array([200], [17]) # 2 * 1 
x = np.arrary([200, 17]) # 1D "vector"
```

### 2.2 Activation vector

```
x = np.array([[200.0, 17.0]])
layer_1 = Dense(units = 3, activation = 'sigmoid')
a1 = layer_1(x)
	tf.Tensor([[0.2 0.7 0.3]], shape = (1, 3), dtype = float32)

a1.numpy()
	array([[0.2, 0.7, 0.3]], dtype = float32)
```



## 3. Building a neural network

### 3.1 Building a neural network architecture

```python
layer_1 = Desne(units=3, activation="sigomid")
layer_2 = Desne(units=1, activation="sigomid")
model = Sequential([layer_1, layer_2])
x = np.array([[200.0, 17.0],
             	[120.0, 5.0]
             	[425.0, 20.0]
             	[212.0, 18.0]]) # 4 * 2
y = np.array([1, 0, 0, 1])
model.compile(...)
model.fit(x, y)

model.predict(x_new)
```



more conventional way to write

```python
model = Sequential([
	Dense(units=3, activation='sigmoid'),
	Dense(units=1, activation='sigmoid')])
```



### 3.2 Digit classification model

```
layer_1 = Dense(units=25, activation="sigmoid")
layer_2 = Dense(units=15, activation="sigmoid")
layer_3 = Dense(units=1, activation="sigmoid")

model = Sequential([layer_1, layer_2, layer_3])
model.compile(...)

x = np.array(...)
y = np.array(...)

model.fit(x, y)
model,predict(x_new) 
```

 

# Neural network implementation in Python

## 1. Forwrd prop in a single layer

$a^{[1]}_1 = g(\vec{w}^{[1]}_1 \cdot \vec{x} + b_1^{[1]})$



if $w^{[2]}_1$ = w2_1

so, 

```python
w1_1 = np.array([1, 2])
b1_1 = np.array([-1])
z1_1 = np.dot(w1_1, x) + b1_1
a1_1 = sigmoid(z1_1)
```



```python
a1 = np.arrary(a1_1, a1_2, a1_3)
```



## 2. General implementation of forward propagation

### 2.1 Forward prop in Numpy

```python
W = np.array([
	[1, -3, 5],
	[2, 4, -6]])
b = np.array([-1, 1, 2])
a_in = np.array([-2, 4])
```

```python
def dense(a_in, W, b):
    units = W.shape[1]
    a_out = np.zeros(units)
    for j in range(units):
        w = W[:,j] # 取出 W 的第 j 列
        z = np.dot(w, a_in) + b[j] # 相当于点乘
        a_out[j] = g(z)
    return a_out
```

```python
def sequential(x):
  a1 = dense(x, W1, b1)
  a2 = dense(x, W2, b2)
  a3 = dense(x, W3, b3)
  a4 = dense(x, W4, b4)
  f_x = a4
  return f_x
```

captial W refer to a matrix

 

# Vectorization

## 1. How neural networks are implemented efficiently

``` python
X = np.array([[200, 17]]) # 2D arrary, 1 by 2
W = np.array([ 
	[1, -3, 5],
	[2, 4, -6]]) # 2 by 3
B = np.array([[-1, 1, 2]])

def dense(A_in, W, B):
	Z = np,matmul(A_in, W) + B # Matrix Multiplication
	A_out = g(Z)
	return A_out
```

## 2. Dot products

### 2.1 Dot products

$$
\mathbf{M} = 
\begin{bmatrix} m_{11} & m_{12} & m_{13} \\
m_{21} & m_{22} & m_{23} 
\end{bmatrix}
$$

example:

$\mathbf{M} = \begin{bmatrix} 1 \\2\end{bmatrix}$ $ \mathbf{N} = \begin{bmatrix} 3 \\4\end{bmatrix}$

$z = (1 \times 3) + (2 \times 4)$


$$
z = \vec{a} \cdot \vec{w} = \vec{a}^{T}\vec{w}
$$

### 2.2 Vector matrix multiplication

