# Classfication



## 1. Motivation

$y$ can only be one of two values: "binary classfication"



false , 0 = "negative class" $\not=$ "bad"



if $f_{w,b}(x) < 0.5 \rightarrow \hat{y} = 0$

if $f_{w,b}(x) > 0.5 \rightarrow \hat{y} = 1$



## 2. Logistic Regression

### 2.1 sigmoid function OR logistic funciton

![](/Users/linyin/study/machine_learning/img/sigmoid_function.png)

Outputs between 0 and 1


$$
g(z) = \frac{1}{1 + e^{-1}} \quad 0 <g(z)<1 
$$
logistic regression
$$
f_{\vec{w},b}(\vec{x}) = g(\vec{w} \cdot \vec{x} + b) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}
$$

### 2.2 Interpretation of logistic regression output

$f_{\vec{w},b}(\vec{x}) = g(\vec{w} \cdot \vec{x} + b) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}$

"probability" that class is 1

$P(y = 0) + P(y = 1) = 1$



Example:

$x$ is "tumor size"

$y$ is 0 (not malignant)

  or 1 (maligant)

$f_{\vec{w},b}(\vec{x}) = 0.7$

means that 70% chance that $y$ is 1

## 3. Decision boundary

When is $f_{\vec{w},b}(\vec{x}) \geq 0.5$ ?

​	$g(z) \geq 0.5$

​	$z \geq 0.5$

​	$\vec{w} \cdot \vec{x} + b \geq 0$

​	$\hat{y} = 1$

 

Decision boundary $z = \vec{w} \cdot \vec{x} + b = 0$

# Cost function

## 1. Cost fucntion for logistic regression

### 1.1 Squared error cost

$$
J(w,b) = \frac{1}{m}\sum_1^m\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2
$$

linear regression

$f_{\vec{w},b}(\vec{x})=\vec{w} \cdot \vec{x} + b$

logistic regression

$f_{\vec{w},b}(\vec{x}) = \frac{1}{e^{-(\vec{w} \cdot \vec{x} + b)}}$

![](/Users/linyin/study/machine_learning/img/Squard_error_cost.png)

if you try to use gradient descent, there are lots of local minima that you can get sucking

### 1.2 Logistic loss function

$$
L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = \begin{cases}
    {-log(f_{\vec{w},b}(\vec{x}^{(i)}))} & \text{if} \ \ y^(i)=1 \\
    {-log(1-f_{\vec{w},b}(\vec{x}^{(i)}))} & \text{if} \ \ y^(i)=0 \\
\end{cases}
$$

Loss is lowest when $f_{\vec{w},b}(\vec{x})$ predicts close to true label $y^{(i)}$.

## 2. Simplified Cost Function

​	 
$$
L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = -y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1 - y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})
$$
if $y^{(i)} = 1$:

$L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = -1\times log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1 - 1)log(1-f_{\vec{w},b}(\vec{x}^{(i)}) = -1 \times log(f(\vec{x^{(i)}}))$



if $y^{(i) = 0}$:

$L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i)) = -0\times log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1 - 0)\times log(1-f_{\vec{w},b}(\vec{x}^{(i)}) = -log(1-f_{\vec{w},b}(\vec{x}^{(i)})$



now,
$$
J(\vec{w}, b) = \frac{1}{m}\sum_{i = 1}^m[L(f_{\vec{w},b}(\vec{x}^{(i)})),y(i))]\\
 = -\frac{1}{m}\sum_{i = 1}^m[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) + (1 - y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})]
$$


