# Multiple Linear Regression 



## 1. Multiple functions

### 1.1 Multiple features (variables)

$X_j = j^{th}$  feature

$n $ = number of features

$\vec{x}^{(i)} = $ features of $i^{th}$ training examples 

$\vec{x}^{(i)}_j = $ value of  feature $j$ in $i^{th}$ training example



### 1.2 Model

Previously: $f_{w, b}(x) = wx + b$

 $f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b$



vector: a list of numbers



## 2. Vectorization

### 2.1 Parameters and features

$\vec{w} = [w_1, w_2,w_3 ]$

$b$ is a number

$\vec{x} = [x_1, x_2,x_3 ]$

linear algebra: count from 1



```python
w = np.array([1.0, 2.5, -3.3])
b = 4
x = np.array([10, 20, 30])
```

code: count from 0 

### 2.2 Without vectorization

 $f_{\vec{w}, b}(\vec{x}) = \sum^n_{j = 1} + b$

```python
f = 0
for j in range(0, n):
	f = f + w[j] * x[j]
f = f + b
```



### 2.3 Vectorization

 $f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b$

```python
f = np.dot(w, x) + b
```

benefict:

1. code shorter
2. code running much faster

because the numpy use parallel hardware

### 2.4 Why faster?

#### 2.4.1 in vectorization: 

run efficient $\rightarrow$ scale to large datasets

#### 2.4.2 in gradient descent

Without vectorization:

```python
for j in range(0, 16):
	w[j] = w[j] - 0.1 * d[j]
```

Vectorization:

```
w = w - 0.1 * dpython
```



## 3. Gradient Descent for Multiplie Regression

### 3.1 Gradient Descent

one feature

$w = w - \alpha\frac{1}{m}\sum_1^m(f_{w, b}(x^{(i)})-\hat{y}^{(i)})x^{(i)}$
	
$b = b - \alpha\frac{1}{m}\sum_1^m(f_{w, b}(x^{(i)})-\hat{y}^{(i)}) $

simultaneously update $w, b$



$n$ features ($n\geq2$)

$w_1 = w_1 - \alpha\frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-\hat{y}^{(i)})x_1^{(i)}$

...

$w_n = w_n - \alpha\frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-\hat{y}^{(i)})x_n^{(i)}$

$b = b - \alpha\frac{1}{m}\sum_1^m(f_{\vec{w}, b}(\vec{x}^{(i)})-\hat{y}^{(i)}) $

simultaneoulsy update $w_j (for j = 1, \cdots, n)\quad and \quad b$

### 3.2 An alternative to gradient descent

Normal equation

* Only for linear regression
* solve for $w, b$ without iterations

 Disadvantages

* Doesn't generalize to other learning algorithms
* Slow when number and features is large ( > 10000)

What you need to know

* Normal equation method may be used in machine learning libraries that implement linear regression
* Gradient descent is the recommended method for finding parameters $w, b



 ## 4 Feature scaling 

$-100 \leq x_1 \leq 100$ , too large $\rightarrow$ rescale

$98.6 \leq x_1 \leq 105$ , too large $\rightarrow$ resclade

$-0.0001 \leq x_1 \leq 0.0010$ , too small $\rightarrow$ rescale



$-2 \leq x_1 \leq 0.3$ , acceptable ranges

### 4.1 Strandardization

$300 \leq x_1 \leq 2000$

$x_{1,scaled} = \frac{x_1}{2000}$

$0.15 \leq x_{1, scaled} \leq 1$

### 4.2 Mean normalization

 $300 \leq x_1 \leq 2000$

May be: $\mu_1 = 600$

$x_{1,scaled} = \frac{x_1 - \mu_1}{2000 - 300}$

 $-0.18 \leq x_1 \leq 0.82$
$$
\mu_j = \frac{1}{m}\sum^{m-1}_{i = 0}x_j^{(i)}
$$




 ### 4.3 Z-score normalization

standard devization $\sigma$

$\sigma_1 = 450$

$x_{1} = \frac{x_1 - \mu_1}{\sigma_1}$

 $-0.67 \leq x_1 \leq 3.1$


$$
\sigma^{2}_j = \frac{1}{m}\sum^{m-1}_{i = 0}(x^{(i)}_j - \mu_j)^2
$$


## 5. Checking gradient descent for convergence

Let $\epsilon$ be $10^{-3}$,

If $J(\vec{w}, b)$ decreases by $\leq \epsilon$  in one iteration,

declare  <span style="color: red">convergence</span>

 (found parameters $\vec{w}, b$ to get close to global minimum)



## 6. Choosing the learning rate

1. cost sometimes small and sometimes big

means you need to use a smaller $\alpha$

2. cost consistently increases after each iteration

such as: $w_1 = w_1 + \alpha d$

use a minus sign: $w_1 = w_1 - \alpha d$

or use a smaller $\alpha$



With a small enough $\alpha$, cost should decrease on every iteration.

If not, that always means there are some bugs in the code



Values of $\alpha$ to try:

... 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 ....

rouhgly three times bigger



## 7. Feature engineering

$x_1$ = frontage

$x_2$ = depth

$f_{\vec{w}, b}(\vec{x}) = w_1x_1 + w_2x_2 + b$



$area = frontage \times depth$

with new feature: $x_3 = x_1x_2$

so: $f_{\vec{w}, b}(\vec{x}) = w_1x_1 + w_2x_2 + w_3x_3 + b$



using intuition to design new features, by transforming or combining original features. 



## 8 . Polynomial Regression

$f_{\vec{w}, b}(\vec{x}) = w_1x + w_2x^2 + w_3x^3 + b$

 $x = 10^3, x^2 = 10^6, x^3 = 10^9$

so, it's important to apply feature scaling to get features into comparable ranges of values

