# Neural_Network_Training



## 2. Training details

### 2.1 Create the model

```python
model = Sequential([
	Dense(units=25, activation='sigmoid'),
	Dense(units=15, activation='sigmoid'),
	Dense(units=1, activation='sigmoid'),
])
```



### 2.2 Loss and cost function

 logistic loss also known as binary cross-entropy

```python
model.compile(loss=BinaryCrossentropy())
```



if you want to solve a regression problem

```python
model.compile(loss=MeanSquaredError())
```



### 2.3 Gradient descent

 compute derivatives for gradient descent using "back propagation"



repeat{

​	$w_j^{[l]} = w_j^{[l]} - \alpha \frac{\partial}{\partial w_j}J(\vec{w},b)$

​	$b_j^{[l]} = b_j^{[l]} - \alpha \frac{\partial}{\partial b}J(\vec{w},b)$

}



```python
model.fit(X, y, epoch=100)
```



# Activation Functions

## 1. Alternatives to the sigmoid activation

### 1.1 ReLU: Recified Linear Unit

$g(z) = max(o, z)$



### 1. 2Linear activation function = "No activation funciton"

$g(z) = z$



### 1.3 Sigmoid

$g(z) = \frac{1}{1 + e^{-z}}$



## 2. Choosing activation functions

### 2.1 Output layer

Binary classification: 

Sigmoid

y = 0/1



Regression:

Linear activation function

y = +/-



Regression:

price of house(never be negative)

ReLU

y = 0 or + 


### 2.2 Hiddle layer

most common choice:

ReLU



First, ReLU is a bit fast to compute



Second, more important, ReLu function goes flat only in the left, 

so the gradient descent won't be very slow. It will has faster learning



### 2.3 Choosing activation summary

output：

1. binary classification: activation = "sigmoid"
2. regression negtive/positve: activation = "linear"
3. regression $y \geq 0$ : activation = "relu"



Hiddle layer:

​	activation="relu"



so,  if the output is classification 

```python
model = Sequential([
	Dense(units=25, activation='relu'),
	Dense(units=15, activation='relu'),
	Dense(units=1, activation='sigmoid'),
])
```



## 3. Why do we need activation functions?

在一段复杂的线段中，ReLU只会符合这条线段的斜率相同的一部分，其他部分是off

如果是Linear的话，就会连斜率不相同的部分也一起覆盖了。

### 3.1 Linear example

$$
a^{[1]} = w^{[1]}_1x + b^{[1]}_1
$$


$$
a^{[2]} = w^{[2]}_1a^{[1]} + b^{[2]}_1\\
				= w^{[2]}_1(w^{[1]}_1x + b^{[1]}_1)+ b^{[2]}_1\\
				= (w^{[2]}_1 + w^{[1]}_1)x + w^{[2]}_1b^{[1]}_1 + b_1^{[2]}\\
				= wx + b
$$
so. $f(x) = wx + b$, is still a linear regression



### 3.2 example

all linear(including output)

equivalent to linear regression



output activation is sigmoid(hidden layers still linear)

equivalent to logistic regression



So, don't use linear activations in hiddle layers

