# Machine learning  overview 



## 1. Machine learning concepts

### -Supervised learning (course 1, 2)

is used most in real-world applications and rapid advancement 

Learn from data **labeled**  with "**right answers**"
$$
x(input) \rightarrow y(output\ lable)
$$
#### 1. Regression

Predict a **number**

**infinitely** many possible outputs

#### 2. Classification 

Predict **categories**

**small number** of possible outputs

### -Unsupervised learning (course 3)

Find something interesting in **unlabeled** data

Data only comes with input x, but not output labels y 

#### 1. clustering 

Group similar data

points together 

#### 2. Anomoly dection

used to detect unusual events

#### 3. Dimensionality reduction

Compress data using fewer numbers

### -Reinforcement learning

### -Practical advice for applying learning algorithms



## 1.2 Linear Regression model

### Notation:

$x$ = "input" variable / feature

$y$ = "output" variable / "target" variable 

$m$ = number of training examples 

$(x, y)$ = single training example 

$(x^{(i)},y^{(i)} )$ = i^th^ training example

$\hat{y}$ = output of the model (prediction / estimated $y$)



how to represent $f$ in this algorithm?

Linear Regression with **one** variable = **Univariate** linear regression  
$$
f_{w, b}(x) = wx + b
$$
Model: $f_{w, b}(x) = wx + b$

$w, b$ : parameters / coefficients / weights



construct a cost function (Squared error cost function):
$$
J(w,b) = \frac{1}{2m}\sum_1^m(y^{(i)}-\hat{y}^{(i)})^2 \\
J(w,b) = \frac{1}{2m}\sum_1^m(f_{w, b}(x^{(i)})-\hat{y}^{(i)})^2
$$
error = $(\hat{y} - y)$



Goal: $\min_{w, b}J(w,b)$




## 1.3 gradient descent

Have some function: $ J(w,b)$

Want: $\min_{w, b}J(w,b)$

*Outline*:

​	start with some $w,b$ (set w = 0, b = 0)

​	Keep changing $w, b$ to reduce $ J(w,b)$

​	Until we settle at or near minimum (may have more than 1 possible minimum)

simultaneously update w and b
$$
tmp\_w = w - \alpha \frac{\partial}{\partial w}J(w,b) \\
tmp\_b = b - \alpha \frac{\partial}{\partial b}J(w,b) \\ 
w = tmp\_w \\ 
b = tmp\_b
$$
$\alpha$ = Learning rate (between 0 and 1)

1. if $\alpha$ is too small: Gradient descent may be slow

2. if $\alpha$ is too big: Gradient descent may:

   -- Overshoot, never reach minimum

   -- Fail to converge, diverge

   

this will Near a local minimum:

​	-- Deravative becomes smaller

​	-- Update steps become smaller 

So it can reach minimum without decreasing learning rate



## 1.4 Linear Regression's gradient descent

in this case:

​	because, $J(w,b) = \frac{1}{2m}\sum_1^m(f_{w, b}(x^{(i)})-\hat{y}^{(i)})^2$

​	so, $\frac{\partial}{\partial w}J(w,b) = \frac{1}{m}\sum_1^m(f_{w, b}(x^{(i)})-\hat{y}^{(i)})x^{(i)} $

​	and, $\frac{\partial}{\partial b}J(w,b) = \frac{1}{m}\sum_1^m(f_{w, b}(x^{(i)})-\hat{y}^{(i)}) $

In fact, a linear regression is a convex function：bowl-shape function 



"Batch": Each step of gradient descent use all the training examples



